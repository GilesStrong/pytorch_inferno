{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "Various functions for statistical inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from pytorch_inferno.model_wrapper import ModelWrapper\n",
    "from pytorch_inferno.callback import PaperSystMod, PredHandler\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import *\n",
    "from collections import OrderedDict\n",
    "from scipy.interpolate import InterpolatedUnivariateSpline\n",
    "import itertools\n",
    "from fastcore.all import partialler\n",
    "from fastprogress import progress_bar\n",
    "import math\n",
    "\n",
    "from torch import Tensor, autograd\n",
    "import torch\n",
    "from torch.distributions import Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def bin_preds(df:pd.DataFrame, bins:np.ndarray=np.linspace(0.,10.,11), pred_name='pred') -> None:\n",
    "    '''Bins predictions over specified range'''\n",
    "    df[f'{pred_name}_bin'] = np.digitize(df[pred_name], bins)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_shape(df:pd.DataFrame, targ:int, bins:np.ndarray=np.linspace(0.,10.,11), pred_name:str='pred_bin') -> Tensor:\n",
    "    r'''Extracts normalised shape of class from binned predictions. Empty bins are filled with a small quantity to avoid zeros.'''\n",
    "    f = df.loc[df.gen_target == targ, pred_name].value_counts(bins=bins-(1/len(bins)))\n",
    "    f.sort_index(inplace=True)\n",
    "    f += 1e-7\n",
    "    f /= f.sum()\n",
    "    return Tensor(f.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_paper_syst_shapes(bkg_data:np.ndarray, df:pd.DataFrame, model:ModelWrapper, bins:np.ndarray=np.linspace(0.,10.,11), pred_cb:PredHandler=PredHandler(),\n",
    "                          r_vals:Tuple[float,float,float]=[-0.2,0,0.2], l_vals:Tuple[float]=[2.5,3,3.5]) -> OrderedDict:\n",
    "    r'''Pass background data through trained model in order to get up/down shape variations.'''\n",
    "    def _get_shape(r,l):\n",
    "        bp = model.predict(bkg_data, pred_cb=pred_cb, cbs=PaperSystMod(r=r,l=l))\n",
    "        n = f'pred_{r}_{l}'\n",
    "        df[n] = df.pred\n",
    "        df.loc[df.gen_target == 0, n] = bp\n",
    "        bin_preds(df, pred_name=n, bins=bins)\n",
    "        return get_shape(df=df, targ=0, bins=np.linspace(0.,len(bins)-1,len(bins)), pred_name=f'{n}_bin')\n",
    "    \n",
    "    shapes = OrderedDict()\n",
    "    for i,r in enumerate(r_vals):\n",
    "        print(f'Running: r={r}')\n",
    "        shapes[f'{i}_{1}'] = _get_shape(r,l_vals[1])\n",
    "    for i,l in enumerate(l_vals):\n",
    "        print(f'Running: l={l}')\n",
    "        shapes[f'{1}_{i}'] = _get_shape(r_vals[1],l)\n",
    "    return OrderedDict((('f_b_nom',shapes['1_1']),\n",
    "                        ('f_b_up', torch.stack((shapes['2_1'],shapes['1_2']))),\n",
    "                        ('f_b_dw', torch.stack((shapes['0_1'],shapes['1_0'])))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_likelihood_width(nll:np.ndarray, mu_scan:np.ndarray, val:float=0.5) -> float:\n",
    "    r'''Compute width of likelihood at 95% confidence-level'''\n",
    "    r = InterpolatedUnivariateSpline(mu_scan, nll-val-nll.min()).roots()\n",
    "    if len(r) != 2: raise ValueError(f'No roots found at {val}, set val to a smaller value.')\n",
    "    return (r[1]-r[0])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def interp_shape(alpha:Tensor, f_b_nom:Tensor, f_b_up:Tensor, f_b_dw:Tensor):\n",
    "    r'''Use quadratic interpolation between up/down systematic shapes and nominal in order to estimate shapes at arbitrary nuisance values.\n",
    "    Linear extrapolation for absolute nuisances values greater than 1 (outside up/down shape range).\n",
    "    Does not account for co-dependence of nuisances.\n",
    "    Adapted from https://github.com/pablodecm/paper-inferno/blob/master/code/template_model.py under BSD 3-clause licence Copyright (c) 2018, Pablo de Castro, Tommaso Dorigo'''\n",
    "    alpha_t = torch.repeat_interleave(alpha.unsqueeze(-1), repeats=f_b_nom.shape[-1], dim=-1)\n",
    "    a = 0.5*(f_b_up+f_b_dw)[None,:]-f_b_nom\n",
    "    b = 0.5*(f_b_up-f_b_dw)[None,:]\n",
    "\n",
    "    switch = torch.where(alpha_t < 0., f_b_dw-f_b_nom, f_b_up-f_b_nom)\n",
    "    abs_var = torch.where(torch.abs(alpha_t) > 1.,\n",
    "                          (b+(torch.sign(alpha_t)*a))*(alpha_t-torch.sign(alpha_t))+switch,\n",
    "                          a*torch.pow(alpha_t, 2)+ b * alpha_t)\n",
    "    return (f_b_nom + abs_var.sum(1, keepdim=True)).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_calc_nll(s_true:float, b_true:float, s_exp:Tensor, f_s:Tensor, alpha:Tensor,\n",
    "             f_b_nom:Tensor, f_b_up:Tensor, f_b_dw:Tensor) -> Tensor:\n",
    "    r'''Unused\n",
    "    Compute multiple negative log-likelihood for specified parameters. Unused due to difficulty of batch-wise hessians in PyTorch.'''\n",
    "    f_b = interp_shape(alpha, f_b_nom, f_b_up, f_b_dw)\n",
    "    t_exp = (s_exp[:,None]*f_s[None,])+(b_true*f_b)\n",
    "    asimov = (s_true*f_s)+(b_true*f_b_nom)\n",
    "    p = torch.distributions.Poisson(t_exp)\n",
    "    return -p.log_prob(asimov).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_diag_grad_hesse(nll:Tensor, alpha:Tensor) -> Tuple[Tensor,Tensor]:\n",
    "    r'''Unused\n",
    "    Compute batch-wise gradient and hessian, but only the diagonal elements.'''\n",
    "    grad = autograd.grad(nll, alpha, torch.ones_like(nll, device=nll.device), create_graph=True)[0]\n",
    "    hesse = autograd.grad(grad, alpha, torch.ones_like(alpha, device=nll.device), create_graph=True, retain_graph=True)[0]\n",
    "    alpha.grad=None\n",
    "    return grad, hesse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_diag_profile(f_s:Tensor, f_b_nom:Tensor, f_b_up:Tensor, f_b_dw:Tensor, n:int,\n",
    "                      mu_scan:Tensor, true_mu:int, n_steps:int=100, lr:float=0.1,  verbose:bool=True) -> Tensor:\n",
    "    r'''Unused\n",
    "    Compute profile likelihood for range of mu values, but only optimise using diagonal hessian elements.'''\n",
    "    alpha = torch.zeros((len(mu_scan),f_b_up.shape[0]), requires_grad=True, device=f_b_nom.device)\n",
    "    f_b_nom = f_b_nom.unsqueeze(0)\n",
    "    get_nll = partialler(parallel_calc_nll, s_true=true_mu, b_true=n-true_mu, s_exp=mu_scan,\n",
    "                         f_s=f_s, f_b_nom=f_b_nom, f_b_up=f_b_up, f_b_dw=f_b_dw)\n",
    "    for i in range(n_steps):  # Newton optimise nuisances\n",
    "        nll = get_nll(alpha=alpha)\n",
    "        grad, hesse = calc_diag_grad_hesse(nll, alpha)\n",
    "        step = torch.clamp(lr*grad.detach()/(hesse+1e-7), -100, 100)\n",
    "        alpha = alpha-step\n",
    "    return get_nll(alpha=alpha), alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def calc_nll(s_true:float, b_true:float, mu:Tensor, f_s_nom:Tensor, f_b_nom:Tensor,\n",
    "             shape_alpha:Optional[Tensor]=None, s_norm_alpha:Optional[Tensor]=None, b_norm_alpha:Optional[Tensor]=None,\n",
    "             f_s_up:Optional[Tensor]=None, f_s_dw:Optional[Tensor]=None,\n",
    "             f_b_up:Optional[Tensor]=None, f_b_dw:Optional[Tensor]=None,\n",
    "             s_norm_aux:Optional[Distribution]=None, b_norm_aux:Optional[Distribution]=None, shape_aux:Optional[List[Distribution]]=None) -> Tensor:\n",
    "    r'''Compute negative log-likelihood for specified parameters.'''\n",
    "    #  Adjust expectation by nuisances\n",
    "    f_s = interp_shape(shape_alpha, f_s_nom, f_s_up, f_s_dw) if shape_alpha is not None and f_s_up is not None else f_s_nom\n",
    "    f_b = interp_shape(shape_alpha, f_b_nom, f_b_up, f_b_dw) if shape_alpha is not None and f_b_up is not None  else f_b_nom \n",
    "    s_exp = mu    +s_norm_alpha.sum() if s_norm_alpha is not None else mu\n",
    "    b_exp = b_true+b_norm_alpha.sum() if b_norm_alpha is not None else b_true\n",
    "    #  Compute NLL\n",
    "    t_exp = (s_exp*f_s)+(b_exp*f_b)\n",
    "    asimov = (s_true*f_s_nom)+(b_true*f_b_nom)\n",
    "    nll = -torch.distributions.Poisson(t_exp).log_prob(asimov).sum()\n",
    "    # Constrain nuisances\n",
    "    if shape_aux is not None:  \n",
    "        if len(shape_aux) != len(shape_alpha): raise ValueError(\"Number of auxillary measurements must match the number of nuisance parameters.\\\n",
    "                                                           Pass `None`s for unconstrained nuisances.\")\n",
    "        for a,x in zip(shape_alpha, shape_aux):\n",
    "            if x is not None: nll = nll-x.log_prob(a)\n",
    "    if b_norm_alpha is not None:\n",
    "        for a,x in zip(b_norm_alpha, b_norm_aux): nll = nll-x.log_prob(a)\n",
    "    if s_norm_alpha is not None:\n",
    "        for a,x in zip(s_norm_alpha, s_norm_aux): nll = nll-x.log_prob(a)\n",
    "    return nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def jacobian(y:Tensor, x:Tensor, create_graph=False):\n",
    "    r'''Compute full jacobian matrix for single tensor. Call twice for hessian.\n",
    "    Copied from https://gist.github.com/apaszke/226abdf867c4e9d6698bd198f3b45fb7 credits: Adam Paszke \n",
    "    TODO: Fix this to work batch-wise (maybe https://gist.github.com/sbarratt/37356c46ad1350d4c30aefbd488a4faa)'''\n",
    "    jac = []                                                                                          \n",
    "    flat_y = y.reshape(-1)                                                                            \n",
    "    grad_y = torch.zeros_like(flat_y)    \n",
    "    for i in range(len(flat_y)):\n",
    "        grad_y[i] = 1.                                                                                \n",
    "        grad_x, = torch.autograd.grad(flat_y, x, grad_y, retain_graph=True, create_graph=create_graph)\n",
    "        jac.append(grad_x.reshape(x.shape))                                                           \n",
    "        grad_y[i] = 0.                                                                                \n",
    "    return torch.stack(jac).reshape(y.shape + x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def calc_grad_hesse(nll:Tensor, alpha:Tensor, create_graph:bool=False) -> Tuple[Tensor,Tensor]:\n",
    "    r'''Compute full hessian and jacobian for single tensor'''\n",
    "    grad = jacobian(nll, alpha, create_graph=True)\n",
    "    hesse = jacobian(grad, alpha, create_graph=create_graph)\n",
    "    return grad, hesse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def calc_profile(f_s_nom:Tensor, f_b_nom:Tensor, n_obs:int, mu_scan:Tensor, mu_true:int,\n",
    "                 f_s_up:Optional[Tensor]=None, f_s_dw:Optional[Tensor]=None,\n",
    "                 f_b_up:Optional[Tensor]=None, f_b_dw:Optional[Tensor]=None,\n",
    "                 shape_aux:Optional[List[Distribution]]=None,\n",
    "                 s_norm_aux:Optional[List[Distribution]]=None, b_norm_aux:Optional[List[Distribution]]=None, nonaux_b_norm:bool=False,\n",
    "                 n_steps:int=100, lr:float=0.1,  verbose:bool=True) -> Tensor:\n",
    "    r'''Compute profile likelihoods for range of mu values, optimising on full hessian.\n",
    "    Ideally mu-values should be computed in parallel, but batch-wise hessian in PyTorch is difficult.'''\n",
    "    for f in [f_s_nom, f_s_up, f_s_dw, f_b_nom, f_b_up, f_b_dw]:  # Ensure correct dimensions\n",
    "        if f is not None and len(f.shape) < 2: f.unsqueeze_(0)\n",
    "    # Cases where nuisance only causes up xor down variation\n",
    "    if (f_s_up is None and f_s_dw is not None): f_s_up = torch.repeat_interleave(f_s_nom, repeats=len(f_s_dw), dim=0)\n",
    "    if (f_s_dw is None and f_s_up is not None): f_s_dw = torch.repeat_interleave(f_s_nom, repeats=len(f_s_up), dim=0)\n",
    "    if (f_b_up is None and f_b_dw is not None): f_b_up = torch.repeat_interleave(f_s_nom, repeats=len(f_b_dw), dim=0)\n",
    "    if (f_b_dw is None and f_b_up is not None): f_b_dw = torch.repeat_interleave(f_s_nom, repeats=len(f_b_up), dim=0)\n",
    "    if f_s_up is not None and f_b_up is not None and len(f_s_up) != len(f_b_up):\n",
    "        raise ValueError(\"Shape variations for signal & background must have the same number of variations. \\\n",
    "                          Please enter the nominal templates for nuisances that only affect either signal of background.\")\n",
    "    # Norm uncertainties\n",
    "    if s_norm_aux is None: s_norm_aux = []\n",
    "    if b_norm_aux is None: b_norm_aux = []\n",
    "    # Compute nuisance indeces\n",
    "    n_alpha = len(f_b_up) if f_b_up is not None else 0\n",
    "    shape_idxs = list(range(n_alpha))\n",
    "    s_norm_idxs = list(range(n_alpha, n_alpha+len(s_norm_aux)))\n",
    "    n_alpha += len(s_norm_aux)\n",
    "    b_norm_idxs = list(range(n_alpha, n_alpha+len(b_norm_aux)+nonaux_b_norm))\n",
    "    n_alpha += len(b_norm_aux)+nonaux_b_norm\n",
    "    \n",
    "    b_true = n_obs-mu_true\n",
    "    if n_alpha > 0:\n",
    "        nlls = []\n",
    "        get_nll = partialler(calc_nll, s_true=mu_true, b_true=b_true,\n",
    "                         f_s_nom=f_s_nom, f_s_up=f_s_up, f_s_dw=f_s_dw,\n",
    "                         f_b_nom=f_b_nom, f_b_up=f_b_up, f_b_dw=f_b_dw,\n",
    "                         s_norm_aux=s_norm_aux, b_norm_aux=b_norm_aux, shape_aux=shape_aux)\n",
    "        for mu in progress_bar(mu_scan, display=verbose):  # TODO: Fix this to run mu-scan in parallel\n",
    "                alpha = torch.zeros((n_alpha), requires_grad=True, device=f_b_nom.device)\n",
    "                for i in range(n_steps):  # Newton optimise nuisances\n",
    "                    nll = get_nll(shape_alpha=alpha[shape_idxs], mu=mu, s_norm_alpha=alpha[s_norm_idxs], b_norm_alpha=alpha[b_norm_idxs])\n",
    "                    grad, hesse = calc_grad_hesse(nll, alpha, create_graph=False)\n",
    "                    step = lr*grad.detach()@torch.inverse(hesse)\n",
    "                    step = torch.clamp(step, -100, 100)\n",
    "                    alpha = alpha-step\n",
    "                nlls.append(get_nll(shape_alpha=alpha[shape_idxs], mu=mu, s_norm_alpha=alpha[s_norm_idxs], b_norm_alpha=alpha[b_norm_idxs]).detach())\n",
    "                if alpha[shape_idxs].abs().max() > 1: print(f'Linear regime: Mu {mu.data.item()}, shape nuisances {alpha[shape_idxs].data}')\n",
    "        nlls = torch.stack(nlls)\n",
    "    else:\n",
    "        nlls = -torch.distributions.Poisson((mu_scan.reshape((-1,1))*f_s_nom)+(b_true*f_b_nom)).log_prob((mu_true*f_s_nom)+(b_true*f_b_nom)).sum(1)\n",
    "    return nlls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
