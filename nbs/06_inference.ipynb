{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from pytorch_inferno.model_wrapper import ModelWrapper\n",
    "from pytorch_inferno.callback import PaperSystMod\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import *\n",
    "from collections import OrderedDict\n",
    "from scipy.interpolate import InterpolatedUnivariateSpline\n",
    "import itertools\n",
    "from fastcore.all import partialler\n",
    "from fastprogress import progress_bar\n",
    "import math\n",
    "\n",
    "from torch import Tensor\n",
    "import torch\n",
    "from torch import autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def bin_preds(df:pd.DataFrame, bins:np.ndarray=np.linspace(0.,1.,11), pred_name='pred') -> None:\n",
    "    df[f'{pred_name}_bin'] = np.digitize(df[pred_name], bins)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_shape(df:pd.DataFrame, targ:int, pred_name:str='pred_bin') -> Tensor:\n",
    "    f = df.loc[df.gen_target == targ, pred_name].value_counts()\n",
    "    f.sort_index(inplace=True)\n",
    "    f /= f.sum()\n",
    "    return Tensor(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_paper_syst_shapes(bkg_data:np.ndarray, df:pd.DataFrame, model:ModelWrapper,\n",
    "                    r_vals:Tuple[float,float,float]=[-0.2,0,0.2], l_vals:Tuple[float]=[2.5,3,3.5]) -> OrderedDict:\n",
    "    def _get_shape(r,l):\n",
    "        bp = model.predict(bkg_data, cbs=PaperSystMod(r=r,l=l))\n",
    "        n = f'pred_{r}_{l}'\n",
    "        df[n] = df.pred\n",
    "        df.loc[df.gen_target == 0, n] = bp\n",
    "        bin_preds(df, pred_name=n)\n",
    "        return get_shape(df, 0, f'{n}_bin')\n",
    "    \n",
    "    shapes = OrderedDict()\n",
    "    for i,r in enumerate(r_vals):\n",
    "        print(f'Running: r={r}')\n",
    "        shapes[f'{i}_{1}'] = _get_shape(r,l_vals[1])\n",
    "    for i,l in enumerate(l_vals):\n",
    "        print(f'Running: l={l}')\n",
    "        shapes[f'{1}_{i}'] = _get_shape(r_vals[1],l)\n",
    "    return OrderedDict((('f_b_nom',shapes['1_1']),\n",
    "                        ('f_b_up', torch.stack((shapes['2_1'],shapes['1_2']))),\n",
    "                        ('f_b_dw', torch.stack((shapes['0_1'],shapes['1_0'])))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_paper_syst_shapes_as_tensor(bkg_data:np.ndarray, df:pd.DataFrame, model:ModelWrapper, r_vals:List[float]=[0], l_vals:List[float]=[3]) -> Tensor:\n",
    "    '''Unused'''\n",
    "    shapes = []\n",
    "    for r,l in itertools.product(r_vals,l_vals):\n",
    "        print(f'Running: r={r} l={l}')\n",
    "        bp = model.predict(bkg_data, cbs=PaperSystMod(r=r,l=l))\n",
    "        n = f'pred_{r}_{l}'\n",
    "        df[n] = df.pred\n",
    "        df.loc[df.gen_target == 0, n] = bp\n",
    "        bin_preds(df, pred_name=n)\n",
    "        shapes.append(get_shape(df, 0, f'{n}_bin'))\n",
    "    return torch.stack(shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def likelihood_from_scan(f_s:Tensor, f_b:Tensor, n:int=1050, mu_scan:np.ndarray=np.linspace(20,80,61), true_mu=50) -> np.ndarray:\n",
    "    '''depreciated'''\n",
    "    asimov = (true_mu*f_s)+((n-true_mu)*f_b)\n",
    "    nll = np.zeros_like(mu_scan)\n",
    "    for i,mu in enumerate(mu_scan):\n",
    "        p = torch.distributions.Poisson((mu*f_s)+((n-mu)*f_b))\n",
    "        nll[i] = -p.log_prob(asimov).sum(1).min()\n",
    "    nll -= nll.min()\n",
    "    return nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_likelihood_width(nll:np.ndarray, val:float=0.5, mu_scan:np.ndarray=np.linspace(20,80,61)) -> float:\n",
    "    \n",
    "    r = InterpolatedUnivariateSpline(mu_scan, nll-val-nll.min()).roots()\n",
    "    if len(r) == 0: raise ValueError(f'No roots found at {val}, set val to a smaller value.')\n",
    "    return (r[1]-r[0])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def interp_shape(alpha:Tensor, f_b_nom:Tensor, f_b_up:Tensor, f_b_dw:Tensor):\n",
    "    alpha_t = torch.repeat_interleave(alpha.unsqueeze(-1), repeats=f_b_nom.shape[0], dim=-1)\n",
    "    a = 0.5*(f_b_up+f_b_dw)-f_b_nom\n",
    "    b = 0.5*(f_b_up-f_b_dw)\n",
    "\n",
    "    switch = torch.where(alpha_t < 0., f_b_dw - f_b_nom, f_b_up - f_b_nom)\n",
    "    abs_var = torch.where(torch.abs(alpha_t) > 1.,\n",
    "                          (2 * b + torch.sign(alpha_t) * a) *\n",
    "                          (alpha_t - torch.sign(alpha_t)) + switch,\n",
    "                          a*torch.pow(alpha_t, 2)+ b * alpha_t)\n",
    "    return f_b_nom + abs_var.sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def calc_nll(s_true:float, b_true:float, s_exp:float, f_s:Tensor, alpha:Tensor,\n",
    "             f_b_nom:Tensor, f_b_up:Tensor, f_b_dw:Tensor) -> Tensor:\n",
    "    f_b = interp_shape(alpha, f_b_nom, f_b_up, f_b_dw)\n",
    "    t_exp = (s_exp*f_s)+(b_true*f_b)\n",
    "    asimov = (s_true*f_s)+(b_true*f_b_nom)\n",
    "    p = torch.distributions.Poisson(t_exp)\n",
    "    return -p.log_prob(asimov).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def calc_grad_hesse(nll:Tensor, alpha:Tensor) -> Tuple[Tensor,Tensor]:\n",
    "    grad = autograd.grad(nll, alpha, create_graph=True)[0]\n",
    "    hesse = autograd.grad(grad, alpha, torch.ones_like(alpha))[0]\n",
    "    alpha.grad=None\n",
    "    return grad, hesse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def calc_profile_nll(s_true:float, b_true:float, s_exp:float, f_s:Tensor, alpha:Tensor,\n",
    "                     f_b_nom:Tensor, f_b_up:Tensor, f_b_dw:Tensor, n_steps:int=100, lr:float=0.1) -> Tuple[Tensor,Tensor]:\n",
    "    get_nll = partialler(calc_nll, s_true=s_true, b_true=b_true, s_exp=s_exp,\n",
    "                         f_s=f_s, f_b_nom=f_b_nom, f_b_up=f_b_up, f_b_dw=f_b_dw)\n",
    "    for i in range(n_steps):  # Newton optimise nuisances\n",
    "        nll = get_nll(alpha=alpha)\n",
    "        grad, hesse = calc_grad_hesse(nll, alpha)\n",
    "        step = lr*grad.detach()/hesse\n",
    "        alpha = alpha-step\n",
    "    return get_nll(alpha=alpha), alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def likelihood_from_updw(f_s:Tensor, f_b_nom:Tensor, f_b_up:Tensor, f_b_dw:Tensor, n:int=1050,\n",
    "                         mu_scan:np.ndarray=np.linspace(20,80,61), true_mu=50, n_steps:int=100, lr:float=0.1) -> np.ndarray:\n",
    "    alpha = torch.zeros((1,f_b_up.shape[0]), requires_grad=True)\n",
    "    opt = partialler(calc_profile_nll, s_true=true_mu, b_true=n-true_mu, f_s=f_s, alpha=alpha,\n",
    "                     f_b_nom=f_b_nom, f_b_up=f_b_up, f_b_dw=f_b_dw, n_steps=n_steps, lr=lr)\n",
    "    nll = np.zeros_like(mu_scan)\n",
    "    for i,mu in enumerate(progress_bar(mu_scan)): nll[i],_ = opt(s_exp=mu)\n",
    "    return nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def calc_profile_nll_random(s_true:float, b_true:float, s_exp:float, f_s:Tensor, alpha:Tensor,\n",
    "                            f_b_nom:Tensor, f_b_up:Tensor, f_b_dw:Tensor, n_steps:int=100, lr:float=0.1) -> Tuple[Tensor,Tensor]:\n",
    "    '''Only for testing'''\n",
    "    get_nll = partialler(calc_nll, s_true=s_true, b_true=b_true, s_exp=s_exp,\n",
    "                         f_s=f_s, f_b_nom=f_b_nom, f_b_up=f_b_up, f_b_dw=f_b_dw)\n",
    "    uni = torch.distributions.Uniform(-2,2)   \n",
    "    alpha = uni.sample((n_steps,f_b_up.shape[0]))\n",
    "    nll_opt,alpha_opt = math.inf, None\n",
    "    for i,a in enumerate(alpha):  # Newton optimise nuisances\n",
    "        nll = get_nll(alpha=a[None,:])\n",
    "        if nll < nll_opt: nll_opt,alpha_opt = nll,a\n",
    "    return nll_opt,alpha_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def likelihood_from_updw_random(f_s:Tensor, f_b_nom:Tensor, f_b_up:Tensor, f_b_dw:Tensor, n:int=1050,\n",
    "                                mu_scan:np.ndarray=np.linspace(20,80,61), true_mu=50, n_steps:int=100, lr:float=0.1) -> np.ndarray:\n",
    "    '''Only for testing'''\n",
    "    alpha = torch.zeros((1,f_b_up.shape[0]), requires_grad=True)\n",
    "    opt = partialler(calc_profile_nll_random, s_true=true_mu, b_true=n-true_mu, f_s=f_s, alpha=alpha,\n",
    "                     f_b_nom=f_b_nom, f_b_up=f_b_up, f_b_dw=f_b_dw, n_steps=n_steps, lr=lr)\n",
    "    nll = np.zeros_like(mu_scan)\n",
    "    for i,mu in enumerate(progress_bar(mu_scan)): nll[i],_ = opt(s_exp=mu)\n",
    "    return nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
