{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp inferno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFERNO loss\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_inferno.model_wrapper import ModelWrapper\n",
    "from pytorch_inferno.callback import *\n",
    "from pytorch_inferno.data import get_paper_data\n",
    "from pytorch_inferno.plotting import *\n",
    "from pytorch_inferno.inference import *\n",
    "from pytorch_inferno.utils import *\n",
    "\n",
    "from fastcore.all import partialler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "from typing import *\n",
    "from collections import OrderedDict\n",
    "from fastcore.all import store_attr\n",
    "from abc import abstractmethod\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch import autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 2000\n",
    "data, test = get_paper_data(200000, bm=0, bs=bs, n_test=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class VariableSoftmax(nn.Softmax):\n",
    "    def __init__(self, temp:float=1, dim:int=-1):\n",
    "        super().__init__(dim=dim)\n",
    "        self.temp = temp\n",
    "        \n",
    "    def forward(self, x:Tensor) -> Tensor: return super().forward(x/self.temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((1,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[2.8165e-06, 5.6471e-03, 2.3061e-13, 9.8380e-01, 3.8285e-17, 7.5622e-04,\n",
       "          2.9185e-15, 9.7832e-03, 6.3891e-06, 3.6494e-09]]),\n",
       " tensor([[3.5602e-02, 1.6289e-01, 1.3618e-03, 4.5721e-01, 2.3886e-04, 1.0896e-01,\n",
       "          5.6828e-04, 1.8181e-01, 4.1939e-02, 9.4184e-03]]),\n",
       " tensor([[0.0785, 0.1680, 0.0154, 0.2814, 0.0064, 0.1374, 0.0099, 0.1774, 0.0852,\n",
       "          0.0404]]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VariableSoftmax(0.1)(x), VariableSoftmax(0.5)(x), VariableSoftmax(1)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(nn.Linear(3,100),  nn.ReLU(),\n",
    "                    nn.Linear(100,100),nn.ReLU(),\n",
    "                    nn.Linear(100,10), VariableSoftmax(0.1))\n",
    "init_net(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y,w = next(iter(data.trn_dl))\n",
    "preds = net(x)\n",
    "assert preds.shape == (bs,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_shape(p:Tensor) -> Tensor:\n",
    "    f = p.sum(0)\n",
    "    f = f/f.sum()\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = y.squeeze()==0\n",
    "f_s = to_shape(preds[~m])\n",
    "f_b = to_shape(preds[m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fcef3a9dc18>]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(to_np(f_s))\n",
    "plt.plot(to_np(f_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    u,d = [],[]\n",
    "    b = x[m]\n",
    "    b[:,0] += 0.2\n",
    "    u.append(to_shape(net(b)))\n",
    "    b[:,0] -= 0.2\n",
    "    b[:,2] *= 3.5/3\n",
    "    u.append(to_shape(net(b)))\n",
    "    b[:,2] /= 3.5/3\n",
    "    b[:,0] -= 0.2\n",
    "    d.append(to_shape(net(b)))\n",
    "    b[:,0] += 0.2\n",
    "    b[:,2] *= 2.5/3\n",
    "    d.append(to_shape(net(b)))\n",
    "    b[:,2] /= 2.5/3\n",
    "    b_up,b_dw = torch.stack(u),torch.stack(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nll,alpha = calc_profile(f_s=f_s, f_b_nom=f_b, f_b_up=b_up, f_b_dw=b_dw, verbose=False, n=1050, mu_scan=torch.linspace(20,80,61), true_mu=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([13.0937, 13.0649, 13.0363, 13.0095, 12.9839, 12.9579, 12.9341, 12.9106,\n",
       "        12.8893, 12.8680, 12.8480, 12.8286, 12.8108, 12.7937, 12.7773, 12.7620,\n",
       "        12.7482, 12.7352, 12.7232, 12.7118, 12.7011, 12.6919, 12.6834, 12.6759,\n",
       "        12.6692, 12.6640, 12.6595, 12.6562, 12.6537, 12.6523, 12.6517, 12.6524,\n",
       "        12.6537, 12.6559, 12.6594, 12.6647, 12.6703, 12.6768, 12.6846, 12.6938,\n",
       "        12.7034, 12.7147, 12.7269, 12.7400, 12.7540, 12.7689, 12.7856, 12.8031,\n",
       "        12.8215, 12.8409, 12.8616, 12.8833, 12.9061, 12.9290, 12.9541, 12.9802,\n",
       "        13.0066, 13.0344, 13.0630, 13.0930, 13.1239], grad_fn=<NegBackward>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nll = nll-nll[nll==nll].min()-0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0579, -0.0868, -0.1154, -0.1421, -0.1678, -0.1937, -0.2175, -0.2410,\n",
       "        -0.2624, -0.2837, -0.3037, -0.3230, -0.3409, -0.3579, -0.3744, -0.3897,\n",
       "        -0.4034, -0.4165, -0.4285, -0.4399, -0.4506, -0.4598, -0.4683, -0.4758,\n",
       "        -0.4825, -0.4876, -0.4922, -0.4955, -0.4980, -0.4994, -0.5000, -0.4993,\n",
       "        -0.4979, -0.4958, -0.4923, -0.4869, -0.4813, -0.4749, -0.4670, -0.4579,\n",
       "        -0.4483, -0.4370, -0.4248, -0.4117, -0.3977, -0.3828, -0.3661, -0.3485,\n",
       "        -0.3301, -0.3108, -0.2900, -0.2684, -0.2456, -0.2227, -0.1975, -0.1715,\n",
       "        -0.1451, -0.1172, -0.0886, -0.0586, -0.0277], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_diff_width(nll:Tensor, mu_scan:np.ndarray) -> Tensor:\n",
    "    def lin_root(nll0,nll1,mu0,mu1):\n",
    "        a = (nll1-nll0)/(mu1-mu0)\n",
    "        b = nll1-(a*mu1)\n",
    "        return -b/a\n",
    "    \n",
    "    u,r,last_mu,last_nll = True,torch.zeros((2)),mu_scan[0],nll[0]\n",
    "\n",
    "    for mu,l in zip(mu_scan[1:],nll[1:]):\n",
    "        if u and l < 0:\n",
    "            r[0] = lin_root(last_nll,l,last_mu,mu)\n",
    "            u = False\n",
    "        elif not u and l > 0:\n",
    "            r[1] = lin_root(last_nll,l,last_mu,mu)\n",
    "            break\n",
    "        if l == l: last_mu,last_nll = mu,l\n",
    "    return r[1]-r[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-17.9897, grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = get_diff_width(nll, mu_scan=np.linspace(20,80,61)); w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class AbsInferno(AbsCallback):\n",
    "    def __init__(self, n:int, mu_scan:Tensor, true_mu:int, n_steps:int=100, lr:float=0.1):\n",
    "        super().__init__()\n",
    "        store_attr()\n",
    "        \n",
    "    def on_train_begin(self) -> None:\n",
    "        r'''\n",
    "        Fake loss function, callback computes loss in `on_forwards_end`\n",
    "        '''\n",
    "        self.wrapper.loss_func = lambda x,y: None\n",
    "        self.profiler = partialler(calc_profile, n=self.n, mu_scan=to_device(self.mu_scan, self.wrapper.device), true_mu=self.true_mu,\n",
    "                                   n_steps=self.n_steps, lr=self.lr, verbose=False)\n",
    "        \n",
    "    @staticmethod\n",
    "    def _to_shape(p:Tensor) -> Tensor:\n",
    "        f = p.sum(0)\n",
    "        f = f + 1e-7\n",
    "        f = f/f.sum()\n",
    "        return f\n",
    "    \n",
    "    @abstractmethod\n",
    "    def _get_up_down(self, x:Tensor) -> Tuple[Tensor,Tensor]: pass\n",
    "        \n",
    "    def _get_diff_width(self, nll:Tensor) -> Tensor:\n",
    "        def lin_root(nll0,nll1,mu0,mu1):\n",
    "            a = (nll1-nll0)/(mu1-mu0)\n",
    "            b = nll1-(a*mu1)\n",
    "            return -b/a\n",
    "\n",
    "        u,r,last_mu,last_nll = True,torch.zeros((2)),self.mu_scan[0],nll[0]\n",
    "        for mu,l in zip(self.mu_scan[1:],nll[1:]):\n",
    "            if u and l < 0:\n",
    "                r[0] = lin_root(last_nll,l,last_mu,mu)\n",
    "                u = False\n",
    "            elif not u and l > 0:\n",
    "                r[1] = lin_root(last_nll,l,last_mu,mu)\n",
    "                break\n",
    "            if l == l: last_mu,last_nll = mu,l\n",
    "        return r[1]-r[0]\n",
    "        \n",
    "    def on_forwards_end(self) -> None:\n",
    "        # Get sig. & bkg. shapes\n",
    "        b = self.wrapper.y.squeeze()==0\n",
    "        f_s = self._to_shape(self.wrapper.y_pred[~b])\n",
    "        f_b = self._to_shape(self.wrapper.y_pred[b])\n",
    "        f_b_up,f_b_dw = self._get_up_down(self.wrapper.x[b])\n",
    "        \n",
    "        # Compute nll\n",
    "        nll,_ = self.profiler(f_s=f_s, f_b_nom=f_b, f_b_up=f_b_up, f_b_dw=f_b_dw)\n",
    "        try: nll = nll-nll[nll==nll].min()-0.5\n",
    "        except RuntimeError: print(nll, self.wrapper.y_pred)\n",
    "        w = self._get_diff_width(nll)\n",
    "        self.wrapper.loss_val = torch.clamp_min(w, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class PaperInferno(AbsInferno):\n",
    "    def __init__(self, n:int=1050, mu_scan:Tensor=torch.linspace(20,80,61), true_mu:int=50, n_steps:int=100, lr:float=0.1):\n",
    "        super().__init__(n=n, mu_scan=mu_scan, true_mu=true_mu, n_steps=n_steps, lr=lr)\n",
    "    \n",
    "    def _get_up_down(self, x:Tensor) -> Tuple[Tensor,Tensor]:\n",
    "        with torch.no_grad():\n",
    "            u,d = [],[]\n",
    "            x[:,0] += 0.2\n",
    "            u.append(self._to_shape(self.wrapper.model(x)))\n",
    "            x[:,0] -= 0.2\n",
    "            x[:,2] *= 3.5/3\n",
    "            u.append(self._to_shape(self.wrapper.model(x)))\n",
    "            x[:,2] /= 3.5/3\n",
    "            x[:,0] -= 0.2\n",
    "            d.append(self._to_shape(self.wrapper.model(x)))\n",
    "            x[:,0] += 0.2\n",
    "            x[:,2] *= 2.5/3\n",
    "            d.append(self._to_shape(self.wrapper.model(x)))\n",
    "            x[:,2] /= 2.5/3\n",
    "            return torch.stack(u),torch.stack(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='1' class='' max='200', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.50% [1/200 00:38<2:06:54]\n",
       "    </div>\n",
       "    \n",
       "\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='progress-bar-interrupted' max='100', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      Interrupted\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 5.862639656066895 Valid: 34.12641174316406\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       device='cuda:0', grad_fn=<NegBackward>) tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-5157f568ad20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m model.fit(200, data=data, opt=partialler(optim.SGD,lr=1e-6), loss=nn.BCELoss(),\n\u001b[0;32m----> 7\u001b[0;31m           cbs=[PaperInferno(),LossTracker(),EarlyStopping(5),GradClip(1e-5)])\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'weights/BCE_Test.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/cernbox/pytorch_inferno/pytorch_inferno/model_wrapper.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, n_epochs, data, opt, loss, cbs)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaster_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0mfit_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcbs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/cernbox/pytorch_inferno/pytorch_inferno/model_wrapper.py\u001b[0m in \u001b[0;36mfit_epoch\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcbs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrn_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcbs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/cernbox/pytorch_inferno/pytorch_inferno/model_wrapper.py\u001b[0m in \u001b[0;36m_fit_batch\u001b[0;34m(self, x, y, w)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcbs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_backwards_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcbs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_backwards_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "net = nn.Sequential(nn.Linear(3,100),  nn.ReLU(),\n",
    "                    nn.Linear(100,100),nn.ReLU(),\n",
    "                    nn.Linear(100,10), VariableSoftmax(0.1))\n",
    "init_net(net)\n",
    "model = ModelWrapper(net)\n",
    "model.fit(200, data=data, opt=partialler(optim.SGD,lr=1e-6), loss=nn.BCELoss(),\n",
    "          cbs=[PaperInferno(),LossTracker(),EarlyStopping(5),GradClip(1e-5)])\n",
    "\n",
    "model.save('weights/BCE_Test.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
