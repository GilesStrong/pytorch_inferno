# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/06_inference.ipynb (unless otherwise specified).

__all__ = ['bin_preds', 'get_shape', 'get_paper_syst_shapes', 'get_likelihood_width', 'interp_shape', 'calc_nll',
           'jacobian', 'calc_grad_hesse', 'calc_profile']

# Cell
from .model_wrapper import ModelWrapper
from .callback import PaperSystMod, PredHandler

import pandas as pd
import numpy as np
from typing import *
from collections import OrderedDict
from scipy.interpolate import InterpolatedUnivariateSpline
import itertools
from fastcore.all import partialler
from fastprogress import progress_bar
import math

from torch import Tensor, autograd
import torch
from torch.distributions import Distribution

# Cell
def bin_preds(df:pd.DataFrame, bins:np.ndarray=np.linspace(0.,10.,11), pred_name='pred') -> None:
    '''Bins predictions over specified range'''
    df[f'{pred_name}_bin'] = np.digitize(df[pred_name], bins)-1

# Cell
def get_shape(df:pd.DataFrame, targ:int, bins:np.ndarray=np.linspace(0.,10.,11), pred_name:str='pred_bin') -> Tensor:
    r'''Extracts normalised shape of class from binned predictions. Empty bins are filled with a small quantity to avoid zeros.'''
    f = df.loc[df.gen_target == targ, pred_name].value_counts(bins=bins-(1/len(bins)))
    f.sort_index(inplace=True)
    f += 1e-7
    f /= f.sum()
    return Tensor(f.values)

# Cell
def get_paper_syst_shapes(bkg_data:np.ndarray, df:pd.DataFrame, model:ModelWrapper, bins:np.ndarray=np.linspace(0.,10.,11), pred_cb:PredHandler=PredHandler(),
                          r_vals:Tuple[float,float,float]=[-0.2,0,0.2], l_vals:Tuple[float]=[2.5,3,3.5]) -> OrderedDict:
    r'''Pass background data through trained model in order to get up/down shape variations.'''
    def _get_shape(r,l):
        bp = model.predict(bkg_data, pred_cb=pred_cb, cbs=PaperSystMod(r=r,l=l))
        n = f'pred_{r}_{l}'
        df[n] = df.pred
        df.loc[df.gen_target == 0, n] = bp
        bin_preds(df, pred_name=n, bins=bins)
        return get_shape(df=df, targ=0, bins=np.linspace(0.,len(bins)-1,len(bins)), pred_name=f'{n}_bin')

    shapes = OrderedDict()
    for i,r in enumerate(r_vals):
        print(f'Running: r={r}')
        shapes[f'{i}_{1}'] = _get_shape(r,l_vals[1])
    for i,l in enumerate(l_vals):
        print(f'Running: l={l}')
        shapes[f'{1}_{i}'] = _get_shape(r_vals[1],l)
    return OrderedDict((('f_b_nom',shapes['1_1']),
                        ('f_b_up', torch.stack((shapes['2_1'],shapes['1_2']))),
                        ('f_b_dw', torch.stack((shapes['0_1'],shapes['1_0'])))))

# Cell
def get_likelihood_width(nll:np.ndarray, mu_scan:np.ndarray, val:float=0.5) -> float:
    r'''Compute width of likelihood at 95% confidence-level'''
    r = InterpolatedUnivariateSpline(mu_scan, nll-val-nll.min()).roots()
    if len(r) != 2: raise ValueError(f'No roots found at {val}, set val to a smaller value.')
    return (r[1]-r[0])/2

# Cell
def interp_shape(alpha:Tensor, f_b_nom:Tensor, f_b_up:Tensor, f_b_dw:Tensor):
    r'''Use quadratic interpolation between up/down systematic shapes and nominal in order to estimate shapes at arbitrary nuisance values.
    Linear extrapolation for absolute nuisances values greater than 1 (outside up/down shape range).
    Does not account for co-dependence of nuisances.
    Adapted from https://github.com/pablodecm/paper-inferno/blob/master/code/template_model.py under BSD 3-clause licence Copyright (c) 2018, Pablo de Castro, Tommaso Dorigo'''
    alpha_t = torch.repeat_interleave(alpha.unsqueeze(-1), repeats=f_b_nom.shape[-1], dim=-1)
    a = 0.5*(f_b_up+f_b_dw)[None,:]-f_b_nom
    b = 0.5*(f_b_up-f_b_dw)[None,:]

    switch = torch.where(alpha_t < 0., f_b_dw-f_b_nom, f_b_up-f_b_nom)
    abs_var = torch.where(torch.abs(alpha_t) > 1.,
                          (b+(torch.sign(alpha_t)*a))*(alpha_t-torch.sign(alpha_t))+switch,
                          a*torch.pow(alpha_t, 2)+ b * alpha_t)
    return (f_b_nom + abs_var.sum(1, keepdim=True)).squeeze(1)

# Cell
def calc_nll(s_true:float, b_true:float, s_exp:Tensor, b_exp:Tensor, f_s:Tensor, alpha:Tensor,
             f_b_nom:Tensor, f_b_up:Optional[Tensor], f_b_dw:Optional[Tensor],
             b_aux:Optional[Distribution], alpha_aux:Optional[List[Distribution]]=None) -> Tensor:
    r'''Compute negative log-likelihood for specified parameters.'''
    f_b = interp_shape(alpha, f_b_nom, f_b_up, f_b_dw) if f_b_up is not None and f_b_dw is not None else f_b_nom
    t_exp = (s_exp*f_s)+(b_exp*f_b)
    asimov = (s_true*f_s)+(b_true*f_b_nom)
    nll = -torch.distributions.Poisson(t_exp).log_prob(asimov).sum()
    if alpha_aux is not None:  # Constrain shape nuisances
        if len(alpha_aux) != len(alpha): raise ValueError("Number of auxillary measurements must match the number of nuisance parameters")
        for a,x in zip(alpha, alpha_aux):
            if x is not None: nll = nll-x.log_prob(a)
    if b_aux is not None: nll = nll-b_aux.log_prob(b_exp)
    return nll

# Cell
def jacobian(y:Tensor, x:Tensor, create_graph=False):
    r'''Compute full jacobian matrix for single tensor. Call twice for hessian.
    Copied from https://gist.github.com/apaszke/226abdf867c4e9d6698bd198f3b45fb7 credits: Adam Paszke
    TODO: Fix this to work batch-wise (maybe https://gist.github.com/sbarratt/37356c46ad1350d4c30aefbd488a4faa)'''
    jac = []
    flat_y = y.reshape(-1)
    grad_y = torch.zeros_like(flat_y)
    for i in range(len(flat_y)):
        grad_y[i] = 1.
        grad_x, = torch.autograd.grad(flat_y, x, grad_y, retain_graph=True, create_graph=create_graph)
        jac.append(grad_x.reshape(x.shape))
        grad_y[i] = 0.
    return torch.stack(jac).reshape(y.shape + x.shape)

# Cell
def calc_grad_hesse(nll:Tensor, alpha:Tensor, create_graph:bool=False) -> Tuple[Tensor,Tensor]:
    r'''Compute full hessian and jacobian for single tensor'''
    grad = jacobian(nll, alpha, create_graph=True)
    hesse = jacobian(grad, alpha, create_graph=create_graph)
    return grad, hesse

# Cell
def calc_profile(f_s:Tensor, f_b_nom:Tensor, f_b_up:Tensor, f_b_dw:Tensor, n:int,
                 mu_scan:Tensor, true_mu:int, n_steps:int=100, lr:float=0.1,  verbose:bool=True,
                 alpha_aux:Optional[List[Distribution]]=None, float_b:bool=False, b_aux:Optional[Distribution]=None) -> Tensor:
    r'''Compute profile likelihoods for range of mu values, optimising on full hessian.
    Ideally mu-values should be computed in parallel, but batch-wise hessian in PyTorch is difficult.
    TODO: Fix this to run mu-scan in parallel'''
    f_b_nom = f_b_nom.unsqueeze(0)
    b_true = n-true_mu
    get_nll = partialler(calc_nll, s_true=true_mu, b_true=b_true, f_s=f_s, f_b_nom=f_b_nom, f_b_up=f_b_up, f_b_dw=f_b_dw,
                         b_aux=b_aux, alpha_aux=alpha_aux)
    nlls = []
    for mu in progress_bar(mu_scan, display=verbose):
        alpha = torch.zeros((f_b_up.shape[0]+float_b), requires_grad=True, device=f_b_nom.device)
        for i in range(n_steps):  # Newton optimise nuisances
            nll = get_nll(alpha=alpha[:-1] if float_b else alpha, s_exp=mu, b_exp=b_true+alpha[-1] if float_b else b_true)
            grad, hesse = calc_grad_hesse(nll, alpha, create_graph=False)
            step = lr*grad.detach()@torch.inverse(hesse)
            step = torch.clamp(step, -100, 100)
            alpha = alpha-step
        nlls.append(get_nll(alpha=alpha[:-1] if float_b else alpha, s_exp=mu, b_exp=b_true+alpha[-1] if float_b else b_true))
        if alpha.abs().max() > 1: print(f'Linear regime: Mu {mu.data.item()}, alpha {alpha.data}')
    return torch.stack(nlls)